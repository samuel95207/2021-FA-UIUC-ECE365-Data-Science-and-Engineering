{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 12: Image denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Due December 4th, 2021 11:59 PM CST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistics and Lab Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [course website](https://courses.engr.illinois.edu/ece365/fa2019/logisticsvvv.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab covers denoising and diffusion maps:\n",
    "\n",
    "- PCA for dimensionality reduction and denoising\n",
    "- Wiener filtering\n",
    "- Diffusion maps\n",
    "\n",
    "The submission procedure is provided below:\n",
    "- You will be provided with a template Python script (main.py) for this lab where you need to implement the provided functions as needed for each question. Follow the instructions provided in this Jupyter Notebook (.ipynb) to implement the required functions. **Do not change the file name or the function headers!**\n",
    "- <b>This lab contains non-programming questions.</b> These questions will be graded manually and their score will appear after the submission deadline. You can write your solutions on paper, scan (or take a photo) of your solutions and upload them as a pdf file under the name `Lab12.pdf`.\n",
    "- Upload <b>BOTH</b> your Python script (.py file) and the pdf file containing your solutions (Lab12.pdf) on Gradescope.\n",
    "- Your grades and feedbacks will appear on Gradescope. The grading for the programming questions is automated using Gradescope autograder, no partial credits are given. Therefore, if you wish, you will have a chance to re-submit your code **within 72 hours** of receiving your first grade for this lab, only if you have *reasonable* submissions before the deadline (i.e. not an empty script).\n",
    "- If you re-submit, the final grade for the programming part of this lab will be calculated as .4 \\* first_grade + .6 \\* .9 \\* re-submission_grade.\n",
    "\n",
    "The purpose of problems titled `Visualization` are to better understand and test your results visually. `Visualization` problems will <b>NOT</b> be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please type all your answers to programming problems in *main.py*. \n",
    "## You can write your solutions for non-programming problems on paper, scan (or take a photo of) your solutions and upload them as a pdf file under the name Lab12.pdf. You need to submit both *main.py* and *Lab12.pdf*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preamble (Don't change this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line runs your python script that you are going to submit. Objects are created for your reference. You can copy-paste this box so you don't need to come up later. **Do not change the file name!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run main.py\n",
    "q1 = Question1()\n",
    "q2 = Question2()\n",
    "q3 = Question3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 : Principal Component Analysis, PCA (20 points)\n",
    "\n",
    "Given a dataset, PCA finds the principal axes of the dataset, those that well describe it. PCA specifies the axes along which the data has the highest variance. To understand PCA, let's look at the following example. This example is taken from Python Data Science Handbook by Jake VanderPlas.\n",
    "\n",
    "We are given a dataset consisting of $200$ points distributed in 2D space. Below we define and plot this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], color='blue')\n",
    "plt.title('Vizualization of the dataset')\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a relationship between the $x$ and $y$ coordinates of the points in the dataset (it seems like there is a linear relationship between the $x$ and $y$ coordinates of the samples of this dataset). Given the above dataset let's see the principal axes specified by PCA. For this purpose, we use `sklearn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "princ_axes = pca.components_\n",
    "princ_std = np.sqrt(pca.explained_variance_) * 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the principal axes alongside the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], color='blue')\n",
    "\n",
    "plt.plot(np.array([0, princ_axes[0, 0] * princ_std[0]]), \n",
    "         np.array([0, princ_axes[1, 0] * princ_std[0]]), \n",
    "         color='red',\n",
    "         label='Principal axis #1')\n",
    "\n",
    "plt.arrow(x=0, y=0, \n",
    "          dx=princ_axes[0, 0] * princ_std[0], dy=princ_axes[1, 0] * princ_std[0], \n",
    "          width=.03, \n",
    "          color='red')\n",
    "\n",
    "plt.plot(np.array([0, princ_axes[0, 1] * princ_std[1]]), \n",
    "         np.array([0, princ_axes[1, 1] * princ_std[1]]), \n",
    "         color='orange', \n",
    "         label='Principal axis #2')\n",
    "\n",
    "plt.arrow(x=0, y=0, \n",
    "          dx=princ_axes[0, 1] * princ_std[1], dy=princ_axes[1, 1] * princ_std[1], \n",
    "          width=.03, \n",
    "          color='orange')\n",
    "\n",
    "plt.title('Vizualization of the dataset + principal axes')\n",
    "plt.legend()\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above visualization shows the principal axes. The length of the axes is related to the variance of the dataset along that axis. Therefore, the dataset has a larger variance along the first principal axis (in red) compared to the second principal axis (in orange). \n",
    "\n",
    "This means we can choose to represent the dataset based on the first principal axis. In other words, we can project the dataset on the first principal axis and discard the remaining part of the data projected on the second principal axis. Let's check this visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the dataset onto the first principal axis\n",
    "pca = PCA(n_components = 1).fit(X)\n",
    "components = pca.transform(X)\n",
    "filtered = pca.inverse_transform(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X[0, 0], X[0, 1], alpha=0.5, color='blue', label='Original data')\n",
    "plt.scatter(filtered[0, 0], filtered[0, 1], color='orange', label='Projected data on 1st princ. axis')\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.5, color='blue')\n",
    "plt.scatter(filtered[:, 0], filtered[:, 1], color='orange')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that after the projection on the first principal axis, the projected data points lie on the first principal axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1-1: PCA for dimensionality reduction (10 points)\n",
    "\n",
    "One application of PCA is reducing the dimentionality of a high-dimensional dataset. This is helpful when we want to visualize a high dimensional dataset and its underlying structure. Let's assume we have a dataset ${X}\\in \\mathbb{R}^{L \\times n}$. This dataset contains $n$ samples. Each of the samples are a vector of length $L$, where $L$ can be larger than $3$. For $L>3$ we cannot visualize this dataset in its original form. Thus, using PCA, we aim to reduce its dimensionality (by projecting the dataset onto a subset of the prinicipal axis).\n",
    "\n",
    "Here we list the PCA steps and how we can represent the dataset on a smaller subspace constructed by $K$ of the principal axes.\n",
    "* Construct the mean of the samples. Let's call this mean $\\overline{X}$. $\\overline{X}$ is a vector of length $L$.\n",
    "* Construct the covarinace matrix as:\n",
    "\\begin{align}\n",
    "C = \\frac{1}{n} (X-\\overline{X}) (X-\\overline{X})^T \n",
    "\\tag 1\n",
    "\\end{align}\n",
    "* Perform eigenvalue decomposition on $C$. This would give you a set of eigenvectors and eigenvalues of $C$.\n",
    "* Choose $K$ top eigenvectors, i.e. the eigenvectors associated with the $K$ largest eigenvalues. These vectores will be your $K$ prinicipal axes. Lets denote these $K$ vectors as $\\{u_k\\}_{k=1}^K$\n",
    "* Find the components of the data samples along the principal axes. For any data sample $x \\in X$ (i.e. $x$ is a column in $X$), this is obtained by:\n",
    "\\begin{align}\n",
    "\\widehat{c} = [<x, u_1>, <x, u_2>, ..., <x, u_K>]\n",
    "\\tag 2\n",
    "\\end{align}\n",
    "where $<., .>$ represents inner product and $\\widehat{c}$ contains the components of $x$ along the top $K$ eigenvectors of $C$.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above descriptions, in this problem, we aim to use PCA to reduce the dimension of a dataset and visualize its structure after dimensionality reduction.\n",
    "\n",
    "Write a function that reduces the dimensionality of a dataset ${X}$ using $K=2$ number of principal axes. Follow the steps described above.\n",
    "\n",
    "Inputs:\n",
    "* The dataset ${X} \\in \\mathbb{R}^{L \\times n}$, a np.ndarray of shape $L \\times n$ where $L$ is the dimension of the data samples (i.e. each data sample is a vector of length $L$) and $n$ is the number of samples.\n",
    "\n",
    "Outputs:\n",
    "* A np.ndarray of shape $2 \\times n$. The output corresponds to the components of the data samples along the first two principal axes.\n",
    "\n",
    "Add your code to `Question1.pca_reduce_dimen` in `main.py`. You are <b>NOT</b> allowed to use `sklearn` library and are asked to implement PCA from scratch. You can compare your implementation with the results from `sklearn` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Testing your Problem 1-1 solution on a dataset with $L=10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "X = scipy.io.loadmat('problem1_1.mat')['X']\n",
    "\n",
    "comps = q1.pca_reduce_dimen(X)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(4, 4))\n",
    "axes.scatter(comps[0, :], comps[1, :])\n",
    "axes.set_title(\"Data in a 2D space\")\n",
    "plt.show()\n",
    "\n",
    "# Do you see any structure in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1-2: PCA for denoising (10 points)\n",
    "Another application of PCA is denoising. Looking back at the example provided at the begining of Problem 1, after projecting the data on the first prinicipal axis we sort of denoised the dataset!\n",
    "\n",
    "In this problem, our goal is to project a dataset containing noisy samples on a subset of the principal axes. This helps in denoising the dataset. To obtain the projection of the dataset on the top $K$ principal axes , you need to follow the same steps as discussed in Problem 1-1, except the last step. You need to replace the last step with:\n",
    "* \n",
    "\\begin{align}\n",
    "\\widehat{x} = \\sum\\limits_{k=1}^K <x, u_k> u_k\n",
    "\\tag 3\n",
    "\\end{align}\n",
    "where $x \\in X$ and $\\widehat{x}$ denotes $x$ projected onto the $K$ principal axes.\n",
    "***\n",
    "Write a function that denoises a noisy dataset by projecting the dataset onto the $K$ principal axes. \n",
    "\n",
    "The inputs:\n",
    "* The dataset $X$, a np.ndarray matrix of size $L \\times n$. Here, $L$ denotes the dimentionality of the samples and $n$ is the number of samples.\n",
    "* $K$, a scalar denoting the number of principal axes.\n",
    "\n",
    "The outputs:\n",
    "* A np.ndarray matrix of size $L \\times n$ containing the denoised samples.\n",
    "\n",
    "Add your code to `Question1.pca_project` in `main.py`. You are <b>NOT</b> allowed to use `sklearn` library and are asked to implement PCA from scratch. You can compare your implementation with the results from `sklearn` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualiziation: Denoising example #1\n",
    "Let's check your implementation on a dataset with $L=15$. Below, we first generate this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a dataset\n",
    "num_samples = 10000 # number of samples in the dataset\n",
    "num_dim = 15\n",
    "mu1 = np.ones((1, num_dim))\n",
    "mu2 = np.ones((1, num_dim))*-1\n",
    "\n",
    "X1 = mu1 + np.random.normal(size=(num_samples//2, num_dim)) * 0.7\n",
    "X2 = mu2 + np.random.normal(size=(num_samples//2, num_dim)) * 0.7\n",
    "X1_clean = mu1 + np.random.normal(size=(num_samples//2, num_dim)) * 0.\n",
    "X2_clean = mu2 + np.random.normal(size=(num_samples//2, num_dim)) * 0.\n",
    "X = np.concatenate((X1, X2), axis=0).T\n",
    "X_clean = np.concatenate((X1_clean, X2_clean), axis=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised = q1.pca_project(X, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(20, 10))\n",
    "count = 0\n",
    "for row in range(3):\n",
    "    for col in range(5):\n",
    "        ind = count * 100\n",
    "        axes[row, col].plot(X[:, ind], label='Noisy sample')\n",
    "        axes[row, col].plot(X_clean[:, ind], label='Clean sample')\n",
    "        axes[row, col].plot(denoised[:, ind], label='PCA denoised')\n",
    "        axes[row, col].set_title('Data sample #{0}'.format(count))\n",
    "        if row==0 and col==0:\n",
    "            axes[row, col].legend()\n",
    "        count +=1 \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After denoising the samples, the denoised samples are closer to the clean samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualiziation: Denoising example #2\n",
    "In this example, we are denoising a set of noisy cryo-EM projection images using our implementation in Problem 1-2. Note that in this example, the projection images are of size $41 \\times 41$. But, we vectorize them before performing PCA. This means for this example the dimensionality of the data samples is $L = 41^2 = 1681$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "mat = scipy.io.loadmat('problem1_2.mat')\n",
    "projs_noisy = mat['projs_noisy']\n",
    "projs_clean = mat['projs_clean']\n",
    "n = int(np.sqrt(projs_noisy.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denoising noisy projection images using PCA\n",
    "# Feel free to change K and see how the denosing results change!\n",
    "K = 10\n",
    "denoised = q1.pca_project(projs_noisy.T, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ind = 0\n",
    "axes[0].imshow(np.reshape(projs_noisy.T[ind, :], [n, n]), cmap='gray')\n",
    "axes[0].set_title('Noisy sample')\n",
    "axes[1].imshow(np.reshape(denoised[ind, :], [n, n]), cmap='gray')\n",
    "axes[1].set_title('PCA denoised')\n",
    "axes[2].imshow(np.reshape(projs_clean.T[ind, :], [n, n]), cmap='gray')\n",
    "axes[2].set_title('Clean sample')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Wiener filtering (20 points)\n",
    "Please refer to the lecture notes to review Wiener filter.\n",
    "\n",
    "### Problem 2-1: Wiener filter expression (10 points)\n",
    "Derive the linear minimum mean squared error estimator of $x$ given the measurements $y = x + \\varepsilon$, where $y, x, \\varepsilon \\in \\mathbb{R}^L$, $\\mathbb{E}[x] = \\mu$ and the covariance of $x$ is $\\Sigma$. The additive noise $\\varepsilon$ is a random Gaussian vector with mean $0_L$ (a vector of zeros of length $L$) and covariance $\\sigma^2 I_{L \\times L}$ ($I_{L \\times L}$ is the identity matrix of size ${L \\times L}$).\n",
    "\n",
    "Add your solution to `Lab12.pdf`. You can write your answer on a paper, take a snapshot of your solution and add it to `Lab12.pdf`. Please mark your solution for this problem with `Problem 2-1`.\n",
    "\n",
    "### Problem 2-2: Wiener filter for image denoising (10 points)\n",
    "Write a function that implements the Wiener filter derived in Problem 2-1. For this problem, assume that $\\mu$, $\\Sigma$ and $\\sigma$ are given.\n",
    "\n",
    "Inputs:\n",
    "* A noisy dataset, a np.ndarray of shape $L \\times n$.\n",
    "* $\\Sigma$, The covariance matrix of $x$, a np.ndarray of shape $L \\times L$.\n",
    "* $\\mu$, the mean of $x$, a np.ndarray of length $L$.\n",
    "* $\\sigma$ the noise standard deviation on each pixel\n",
    "\n",
    "Outputs:\n",
    "* Wiener filtered samples, a np.ndarray of shape $L \\times n$.\n",
    "\n",
    "Add your code to `Question2.wiener_filter` in `main.py`. You are <b>NOT</b> allowed to use pre-defined functions in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Image denoising\n",
    "Let's check the performance of the Wiener filter on a cryo-EM projection image denoising task. For this purpose, you are given the mean vector $\\mathbb{E}[x] = \\mu$, the covariance matrix of the clean data $\\Sigma$ and $\\sigma$. Let's first load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "mat = scipy.io.loadmat('problem2_2.mat')\n",
    "mu = mat['mu']\n",
    "C = mat['C']\n",
    "sigma = mat['sigma']\n",
    "projs_noisy = mat['projs_noisy']\n",
    "projs_clean = mat['projs_clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we denoise the noisy projection images using the Wiener filter. We visualize the noisy, filtered and clean samples below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_imgs = q2.wiener_filter(projs_noisy, C, mu, sigma)\n",
    "ind = 0\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(np.reshape(projs_noisy[:, ind], [n, n]), cmap='gray')\n",
    "axes[0].set_title('Noisy image')\n",
    "axes[1].imshow(np.reshape(filtered_imgs[:, ind], [n, n]), cmap='gray')\n",
    "axes[1].set_title('Wiener-filtered image')\n",
    "axes[2].imshow(np.reshape(projs_clean[:, ind], [n, n]), cmap='gray')\n",
    "axes[2].set_title('Clean image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional (no points): If you are not given $\\sigma$ and $\\mu$, can you estimate it from the noisy dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Diffusion maps (30 points)\n",
    "Diffusion maps, similar to PCA, are used for dimensionality reduction and data analysis. In this problem, we assume we are given the Markov transition matrix $A$ and we practice some concepts discussed in the lecture.\n",
    "\n",
    "### Problem 3-1: Derivation of Eigenvectors and values of a ring graph (10 points)\n",
    "Assume a ring graph with $n$ vertices. The transition matrix of this graph is of size $n \\times n$ and has the following form:\n",
    "\\begin{align}\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "0& \\frac{1}{2}& 0& \\ldots & 0 & \\frac{1}{2} \\\\\n",
    "\\frac{1}{2}& 0& \\frac{1}{2}& 0 & \\ldots & 0 \\\\\n",
    "0 & \\frac{1}{2}& 0& \\frac{1}{2}& \\ddots & \\vdots \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & 0 & 0 \\\\\n",
    "0 & \\ldots & 0 & \\frac{1}{2} & 0 & \\frac{1}{2} \\\\\n",
    "\\frac{1}{2} & 0 & \\ldots & 0 & \\frac{1}{2} & 0\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Show that the vectors $\\{v_\\ell\\}_{\\ell=1}^n$ given by:\n",
    "\\begin{align}\n",
    "v_\\ell(r) = e^{2 \\pi i r \\ell/n}\n",
    "\\end{align}\n",
    "are the eigenvectors of $A$. Also, derive the eigenvalues of $A$.\n",
    "\n",
    "Add your solution to `Lab12.pdf`. You can write your answer on a paper, take a snapshot of your solution and add it to `Lab12.pdf`. Please mark your solution for this problem with `Problem 3-1`.\n",
    "\n",
    "### Problem 3-2: Derivation of Eigenvectors and values of a complete graph (10 points)\n",
    "Assume a complete graph with $n$ vertices. The transition matrix of this graph is of size $n \\times n$ and has the following form:\n",
    "\\begin{align}\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "0& \\frac{1}{n-1}& \\frac{1}{n-1}& \\ldots & \\frac{1}{n-1} & \\frac{1}{n-1} \\\\\n",
    "\\frac{1}{n-1}& 0& \\frac{1}{n-1}& \\frac{1}{n-1} & \\ldots & \\frac{1}{n-1} \\\\\n",
    "\\frac{1}{n-1} & \\frac{1}{n-1}& 0& \\frac{1}{n-1}& \\ddots & \\vdots \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\frac{1}{n-1} & \\frac{1}{n-1} \\\\\n",
    "\\frac{1}{n-1} & \\ldots & \\frac{1}{n-1} & \\frac{1}{n-1} & 0 & \\frac{1}{n-1} \\\\\n",
    "\\frac{1}{n-1} & \\frac{1}{n-1} & \\ldots & \\frac{1}{n-1} & \\frac{1}{n-1} & 0\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Derive the eigenvalues of the matrix $A$ for a complete graph.\n",
    "\n",
    "Add your solution to `Lab12.pdf`. You can write your answer on a paper, take a snapshot of your solution and add it to `Lab12.pdf`. Please mark your solution for this problem with `Problem 3-2`.\n",
    "\n",
    "### Problem 3-3: Eigenvectors and eigenvalues of a graph (10 points)\n",
    "Write a function that given $A$, outputs the top-$(n-1)$ non-trivial eigenvectors and all eigen-values of $A$ in descending order. Note that, these eigenvectors constitute the embedding of this graph.\n",
    "\n",
    "Inputs: \n",
    "* $A$ a $n \\times n$ np.ndarray denoting the Markov transition matrix of the graph.\n",
    "\n",
    "Outputs:\n",
    "* Sorted eigenvalues of $A$ in decsending order, a np.ndarray of length $n$.\n",
    "* Top $n-1$ non-trivial eigenvectors, a np.ndarray of shape $n \\times n-1$.\n",
    "\n",
    "Add your code to `Question3.embedding` in `main.py`. You can check if the results of this function match your derivations in Problem 3-1 and 3-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Embedding of a ring graph\n",
    "Test your code on ring and complete graphs and check if the results matches with your derivations in problem 3-1 and 3-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_ring_graph(mat_sz):\n",
    "    # ring graph\n",
    "    row1 = np.zeros((mat_sz,))\n",
    "    row1[1] = 0.5\n",
    "    row1[-1] = 0.5\n",
    "    W = scipy.linalg.circulant(row1)\n",
    "    print(W)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_complete_graph(mat_sz):\n",
    "    W = np.ones((mat_sz, mat_sz)) - np.eye(mat_sz)\n",
    "    W /= (mat_sz-1)\n",
    "    print(W)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 12\n",
    "W = construct_ring_graph(n)\n",
    "v, w = q3.embedding(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.scatter(v[:, 0], v[:, 1])\n",
    "plt.title('Embedding of ring graph')\n",
    "ax.set_aspect('equal')\n",
    "plt.show()\n",
    "\n",
    "# eigen-value plots\n",
    "plt.figure()\n",
    "plt.plot(w, marker='x')\n",
    "plt.ylabel('Eigen values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Embedding of a complete graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete graph\n",
    "n = 3\n",
    "W = construct_complete_graph(n)\n",
    "v, w = q3.embedding(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.scatter(v[:, 0], v[:, 1])\n",
    "plt.title('Embedding of the complete graph')\n",
    "ax.set_aspect('equal')\n",
    "plt.show()\n",
    "\n",
    "# eigen-value plots\n",
    "plt.figure()\n",
    "plt.plot(w, marker='x')\n",
    "plt.ylabel('Eigen values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And this concludes Lab 12! Congratulations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

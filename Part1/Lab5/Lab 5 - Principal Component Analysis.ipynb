{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 - Principal Component Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Due: September 29, 2021 at 11:59 PM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistics and Lab Submission**\n",
    "\n",
    "See the course website. This is the last lab for this section of the course. Make sure to be up to date for the policies of the second part of the course. **You will have another lab next week and a different TA (who is not familiar with this lab), so it is in your best interests to finish this lab before next week's lab session.**\n",
    "\n",
    "The submission procedure is provided below:\n",
    "- You will be provided with a template Python script (main.py) for this lab where you need to implement the provided functions as needed for each question. Follow the instructions provided in this Jupyter Notebook (.ipynb) to implement the required functions. **Do not change the file name or the function headers!**\n",
    "- Upload only your Python script (.py file) on Gradescope. Don't upload your datasets or Jupyter Notebook (.ipynb file).\n",
    "- Your grades and feedbacks will appear on Gradescope. The grading for the programming questions is automated using Gradescope autograder, no partial credits are given. Therefore, if you wish, you will have a chance to re-submit your code **within 72 hours** of receiving your first grade for this lab, only if you have *reasonable* submissions before the deadline (i.e. not an empty script).\n",
    "- If you re-submit, the final grade for the programming part of this lab will be calculated as .4 \\* first_grade + .6 \\* .9 \\* re-submission_grade.\n",
    "- This lab also has Multiple Choice Questions (MCQs) that are needed to be completed on Gradescope **within the deadline**.\n",
    "\n",
    "There are some problems which have short answer questions. They are not graded, but we are free to discuss answers to these problems. **Multiple Choice Questions (MCQs) will be graded on Gradescope!**\n",
    "\n",
    "Remember in many applications, the end goal is not always \"run a classifier\", like in a homework problem, but is to use the output of the classifier in the context of the problem at hand (e.g. detecting spam, identifying cancer, etc.). Because of this, some of our Engineering Design-type questions are designed to get you to think about the entire design problem at a high level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What You Will Need To Know For This Lab:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eigendecomposition\n",
    "- Singular Value Decomposition\n",
    "- Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble (Don't change this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "# %matplotlib inline\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "from sklearn import neighbors\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy.spatial.distance as dist\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run main.py\n",
    "q1 = Question1()\n",
    "q2 = Question2()\n",
    "q3 = Question3()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Interactive Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably already included in magic matplotlib notebook\n",
    "# from IPython.display import display\n",
    "# from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Visualizing Principal Components (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will be implementing PCA, visualizing the principal components and using it to perform dimensionality reduction. \n",
    "\n",
    "Do not use a pre-written implementation of PCA for this problem (e.g. sklearn.decomposition.PCA). You should assume that the input data has been appropriately pre-processed to have zero-mean features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will generate some data.\n",
    "numpy.random.seed(seed=2232017)\n",
    "true_cov = np.array([[1,.5,.2],[.5,1,.3],[.2,.3,1]])\n",
    "data=np.random.randn(1000,3).dot(np.linalg.cholesky(true_cov).T) \n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we visualize the data using a 3D scatterplot. \n",
    "\n",
    "Our data is stored in a variable called `data` where each row is a feature vector (with three features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = Axes3D(fig,auto_add_to_figure=False)\n",
    "fig.add_axes(ax)\n",
    "ax.scatter(data[:,0],data[:,1],data[:,2])\n",
    "# @interact(elev=(-90, 90), azim=(0, 360))\n",
    "# def view(elev, azim):\n",
    "#     ax.view_init(elev, azim)\n",
    "#     display(ax.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all parts below, you can check out algorithm 9 on textbook pages 82-83."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1:** Write function `pca` which implements PCA. You can implement this via either the eigendecomposition of the SVD. <b>(20 points)</b>\n",
    "\n",
    "You will be given as input:\n",
    "- A $(N,d)$ numpy array of data (with each row as a feature vector)\n",
    "\n",
    "Your function should return a tuple consisting of the PCA transformation matrix (which is $(d,d)$), and a vector consisting of the amount of variance explained in the data by each PCA feature. Note that each **row** of the $(d,d)$ matrix should contain a principal component. Also note that the PCA features are ordered in **decreasing** amount of variance explained, by convention and as you need to pass the autograder.\n",
    "\n",
    "Hints:\n",
    "- If you use eigen-decomposition, the function <a href=\"http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.linalg.eigh.html\">numpy.linalg.eigh</a> will be useful. Note that it returns its eigenvalues (and corresponding eigenvectors) in **ascending** order. `numpy.fliplr` or similar may be useful as well. Besides, the easiest way to flip a numpy vector x is by x\\[::-1\\].\n",
    "- You can calculate the covariance matrix of the data by multiplying the data matrix with its transpose in the appropriate order, and scaling it. \n",
    "- If you use SVD, the function <a href=\"http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.linalg.svd.html\">numpy.linalg.svd</a> will be useful. Use the full SVD (default). Be careful with how the SVD is returned in `numpy.linalg.svd` (`V` in numpy is the transpose of what is in the notes). \n",
    "- Do NOT use `numpy.cov`-- we are assuming the data has zero mean beforehand, so the number of degrees of freedom is different (since the covariance estimate knows the mean in our case). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will run PCA on your data, store your PCA transformation in a variable called `W`, and the amount of variance explained by each PCA feature in a variable called `s`, and print out the principal components (i.e. the rows of `W`) along with the corresponding amount of variance explained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = Question1()\n",
    "W,s = q1.pca(data)\n",
    "\n",
    "# This will print out the principal components & the amount of variance they explain\n",
    "for i in range(W.shape[1]):\n",
    "    print(i+1,end='')\n",
    "    print('-th principal component: ', W[i,:], \"\\t Variance:\",s[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see your explained variance decreasing above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the principal components on top of our data. The first principal component is in red, and captures the most variance. The second principal component is in green, while the last principal component is in yellow.\n",
    "\n",
    "We generated our data from am *elliptical distribution*, so it should be easy to visualize these components as the axes of the data (which looks like an ellipsoid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figb = plt.figure()\n",
    "axb = Axes3D(figb,auto_add_to_figure=False)\n",
    "figb.add_axes(axb)\n",
    "axb.scatter(data[:,0],data[:,1],data[:,2],alpha=0.1)\n",
    "c=['r-','g-','y-']\n",
    "for var, pc,color in zip(s, W,c):\n",
    "    axb.plot([0, 2*var*pc[0]], [0, 2*var*pc[1]], [0, 2*var*pc[2]], color, lw=2)\n",
    "# @interact(elev=(-90, 90), azim=(0, 360))\n",
    "# def view(elev, azim):\n",
    "#     axb.view_init(elev, azim)\n",
    "#     display(axb.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If done correctly, the red line should be longer than the green line which should be longer than the yellow line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will implement functions to generate PCA features.\n",
    "\n",
    "**Part 2:** Write function `pcadimreduce` which implements dimension reduction via PCA. It takes in three inputs:\n",
    "- A $(N,d)$ numpy array, `data`, with each row as a feature vector\n",
    "- A $(d,d)$ numpy array, `W`, the PCA transformation matrix (e.g. generated from `pca`)\n",
    "- A number `k` ($\\leq d$), which is the number of PCA features to retain\n",
    "\n",
    "It should return a $(N,k)$ numpy array, where the $i$-th row contains the PCA features corresponding to the $i$-th input feature vector. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3:** Write function `pcareconstruct` which reconstructs the original features from the PCA features. It takes in two inputs:\n",
    "- A $(N,k)$ numpy array, `pcadata`, with each row as a PCA feature vector (e.g. generated from `pcadimreduce`)\n",
    "- A $(d,d)$ numpy array, `W`, the PCA transformation matrix (e.g. generated from `pca`)\n",
    "\n",
    "It should return a $(N,d)$ numpy array, where the $i$-th row contains the reconstruction of the original $i$-th input feature vector (in `data`) based on the PCA features contained in `pcadata`. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = Question1()\n",
    "# Reconstructed data using all the principal components\n",
    "reduced_data=q1.pcadimreduce(data,W,3)\n",
    "reconstructed_data=q1.pcareconstruct(reduced_data,W)\n",
    "\n",
    "print (\"This should be small:\",np.max(np.abs(data-reconstructed_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, if you take $k=3$, perform dimensionality reduction then reconstruction, you should get the original data back:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One use of PCA is to help visualize data. The 3-D plots above are a bit hard to read on a 2-D computer screen or when printed out. \n",
    "\n",
    "The following code uses PCA to to reduce the data to $k$ dimensions, and constructs an approximation of the original features using the first $k$ principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = q1.pcadimreduce(data,W,2)\n",
    "scatter(reduced_data[:,0],reduced_data[:,1],s=2)\n",
    "reconstructed_data = q1.pcareconstruct(reduced_data,W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the data using two principal components in the original feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figc = plt.figure()\n",
    "axc = Axes3D(figc,auto_add_to_figure=False)\n",
    "figc.add_axes(axc)\n",
    "axc.scatter(reconstructed_data[:,0],reconstructed_data[:,1],reconstructed_data[:,2],alpha=0.1)\n",
    "c=['r-','g-','y-']\n",
    "for var, pc,color in zip(s, W,c):\n",
    "    axc.plot([0, 2*var*pc[0]], [0, 2*var*pc[1]], [0, 2*var*pc[2]], color, lw=2)    \n",
    "# @interact(elev=(-90, 90), azim=(0, 360))\n",
    "# def view(elev, azim):\n",
    "#     axc.view_init(elev, azim)\n",
    "#     display(axc.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If done correctly, you should see no component of the data along the third principal direction, and the data should lie in a plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses PCA to reduce the data to one dimension and store the one dimensional PCA feature in `reduced_data_1` and constructs an approximation of the original features using the first  principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data_1 = q1.pcadimreduce(data,W,1)\n",
    "reconstructed_data_1 = q1.pcareconstruct(reduced_data_1,W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize this in the original feature space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figd = plt.figure()\n",
    "axd = Axes3D(figd,auto_add_to_figure=False)\n",
    "figd.add_axes(axd)\n",
    "axd.scatter(reconstructed_data_1[:,0],reconstructed_data_1[:,1],reconstructed_data_1[:,2],alpha=0.1)\n",
    "c=['r-','g-','y-']\n",
    "for var, pc,color in zip(s, W,c):\n",
    "    axd.plot([0, 2*var*pc[0]], [0, 2*var*pc[1]], [0, 2*var*pc[2]], color, lw=2)\n",
    "    \n",
    "# @interact(elev=(-90, 90), azim=(0, 360))\n",
    "# def view(elev, azim):\n",
    "#     axd.view_init(elev, azim)\n",
    "#     display(axd.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If done correctly, you should see no component of the data along the second and third principal direction, and the data should lie along a line. This may be easier with the Interactive Mode on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the PCA feature as a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure()\n",
    "n, bins, patches = hist(reduced_data_1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 2: PCA for Data Compression (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class, you saw an example application of PCA to create eigenfaces. In this part of the lab, we will look at eigenfaces for compression using the <a href=\"http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\">Olivetti faces dataset</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import get_data_home\n",
    "print(get_data_home())\n",
    "# If you get an error below, you need to delete olivetti_py3.pkz in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we load the Olivetti dataset\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "oli = fetch_olivetti_faces()\n",
    "# If you get an error here, please delete ~/scikit_learn_data/olivetti_py3.pkz (if on Mac by default, otherwise call get_data_home() to get the default package location)\n",
    "# For details, see https://github.com/scikit-learn/scikit-learn/issues/19376#issuecomment-777636008\n",
    "    \n",
    "# Height and Width of Images are in h,w. You will need to reshape them to this size display them.\n",
    "h = 64\n",
    "w = 64\n",
    "X = oli.data\n",
    "\n",
    "X_t = X[20]\n",
    "X = X[:-1]\n",
    "\n",
    "# This centering is unnecessary. It just makes the pictures a bit more readable. \n",
    "X_m = np.mean(X,axis=0)\n",
    "X = X - X_m # center them\n",
    "X_t = X_t - X_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole data set is in `X`. You will compress a specific image `X_t`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the Olivetti Faces:\n",
    "<img src=\"olivettifaces.gif\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be making use of Scikit-Learn's <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">PCA</a> functionality. \n",
    "\n",
    "In particular, three functions will be useful for this problem :\n",
    "- `PCA.fit` : Finds the requested number of principal components.\n",
    "- `PCA.transform` : Apply dimensionality reduction (returns the PCA features)\n",
    "- `PCA.inverse_transform` : Go from PCA features to the original features (Useful for visualizing)\n",
    "\n",
    "You will also find the following useful:\n",
    "- `PCA.explained_variance_ratio_` : Fraction of variance explained by each of the principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1:** Write function `unexp_var` that will fit a PCA on X, and return the fraction of **unexplained** variance on `X` by PCA retaining the first $i$ principal components, for $i=1,\\ldots,k$. You will also return the pca object that has been fit on the image X.\n",
    "\n",
    "The following code uses this function to plot the fraction of unexplained variance. Your result should be a scree plot (normalized by the total variance). \n",
    "\n",
    "Hint: `numpy.cumsum` may be useful for this. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca, unexpv = q2.unexp_var(X,200)\n",
    "\n",
    "figure()\n",
    "plot(np.arange(1,201),unexpv)\n",
    "xlabel('Number of Principal Components')\n",
    "ylabel('Fraction of unexplained variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code visualizes the first 5 principal components as well as the 30th, 50th and 100th principal components, which are called **eigenfaces** in this context. Our PCA object is called `pca`, and the eigenfaces are contained in `pca.components_`, where each row is a principal component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn=np.asarray([0,1,2,3,4,29,49,99])\n",
    "\n",
    "figure(figsize=(8,16))\n",
    "for i in range(fn.size):\n",
    "    subplot(4,2,i+1)\n",
    "    title(\"{}-th principal component: \".format(fn[i]))\n",
    "    imshow((pca.components_[fn[i]]).reshape((h,w)),cmap=cm.Greys_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later eigenfaces capture more detail as compared to earlier ones (e.g. they're specific to some guy). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, you will compress an image, `X_t`, using PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what X_t looks like:\n",
    "figure()\n",
    "imshow((X_t).reshape((h,w)),cmap=cm.Greys_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2:** Write function `pca_approx` that will take as input the `pca` object fit on `X` and returned in the previous part, the input `X_t`, and `i`, and returns an approximation of `X_t` using the the first `i`  principal components (learned from `X`).\n",
    "\n",
    "You can employ the following procedure:\n",
    " \n",
    "1. Transform `X_t` to the PCA features determined by `X`.\n",
    "2. Retain the first `i` PCA features of the transformed `X_t` (by setting the rest to zero). \n",
    "3. Transform the result from step 2 back to the original feature space. \n",
    "\n",
    "Hint: You would want to reshape your `X_t` when using `pca.transform` and `pca.inverse_transform` to (1,-1) since it is a single image.\n",
    "\n",
    "The following code displays the image in `X_t`'s approximation using the first `i` principal components (learned from `X`)  where i=1,10,20,...,100 (i.e. in increments of 10), then 120,140,160,180,200 (i.e. in increments of 20).\n",
    "\n",
    "\n",
    "<b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes=np.array([1] + [i for i in range(10,101,10)] + [i for i in range(120,201,20)])\n",
    "figure(figsize=(8,22))\n",
    "for i in range(sizes.size):\n",
    "    recon_img=q2.pca_approx(X_t, pca, sizes[i])\n",
    "    subplot(8,2,i+1)\n",
    "    title(\"{} principal components used: \".format(sizes[i]))\n",
    "    imshow((recon_img).reshape((h,w)),cmap=cm.Greys_r)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: PCA for Classification (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load a data set of digits drawn from zip codes written on US mail. This data set was designed to help get good algorithms to sort mail by zip code automatically. It has been preprocessed a bit, with details given <a href=\"http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip.info.txt\">here</a>. Each feature vector consists of real values representing grayscale values of a 16 by 16 image of a digit. The training data has 7291 samples, while the validation data has 2007 samples. Note that this is not the same dataset built into scikit- learn -- it is much larger.\n",
    "\n",
    "Use `sklearn.decomposition.PCA` for this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Data\n",
    "#Read in the Training Data\n",
    "traindata_tmp= np.genfromtxt('zip.train', delimiter=' ')\n",
    "#The training labels are stored in \"trainlabels\", training features in \"traindata\"\n",
    "trainlabels=traindata_tmp[:,0]\n",
    "traindata=traindata_tmp[:,1:]\n",
    "#Read in the Validation Data\n",
    "valdata_tmp= np.genfromtxt('zip.val', delimiter=' ')\n",
    "#The validation labels are stored in \"vallabels\", validation features in \"valdata\"\n",
    "vallabels=valdata_tmp[:,0]\n",
    "valdata=valdata_tmp[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in Lab 2, you found that the validation error on this data set was 0.056 for 1-NN. \n",
    "\n",
    "Write function `pca_classify` that returns the validation errors using 1-NN on the PCA features using 1,2,...,k PCA features, the minimum validation error, and number of PCA features used. <b>(20 points)</b>\n",
    "\n",
    "Hint:\n",
    "- While returning the number of features, remember that the number of features starts from 1 and not from 0 (i.e., be careful while using the array indexing).\n",
    "- This function should take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ve, min_ve, pca_feat = q3.pca_classify(traindata, trainlabels, valdata, vallabels, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will plot the validation error vs the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure()\n",
    "plot(np.arange(1,256+1),ve)\n",
    "xlabel('Number of PCA features')\n",
    "ylabel('Validation Error')\n",
    "print (\"Minimum Validation error: \",min_ve)\n",
    "print (\"Number of PCA features to retain:\",pca_feat)\n",
    "axis(\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 (For your own understanding): Spectral Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Problem 4 is optional and for your own understanding. It will not be graded by the autograder, and will not count towards your score for this lab.</b>\n",
    "\n",
    "In this problem, you will implement a powerful clustering algorithm known as spectral clustering. It can separate data that in some cases, K-means cannot (as you will see in this problem).\n",
    "\n",
    "Spectral clustering works by forming a graph based on similarities between data vectors, and looking for cluster of data vectors such that the similarity between them is high, but the similarity to vectors outisde the clusters is low (and the clusters aren't too small).\n",
    "\n",
    "See Section 4.3 in the notes for details on how it works, or [this tutorial](https://arxiv.org/abs/0711.0189). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Spectral Clustering Algorithm (Alg. 9):**\n",
    "\n",
    "\n",
    "1. Let $\\tilde{L} = I - D^{-1/2} S D^{-1/2}$ where $D^{-1/2}$ is a square diagonal matrix with $\\frac{1}{\\sqrt{d_i}}$ as the $i$-th entry on the diagonal (where $\\mathbf{d} = S \\mathbf{1}$). \n",
    "2. Take the eigen-decomposition of $\\tilde{L}= U \\Lambda U^\\top$ where $\\Lambda$ is a diagonal matrix containing the eigenvalues of $L$.\n",
    "3. Let $U_K$ be a matrix whose columns are the eigenvectors corresponding to the $K$-smallest eigenvalues of $L$.\n",
    "4. Normalize each row of $U_K$ (i.e. divide each entry on the $i$-th row by the norm of the $i$-th row)\n",
    "5. Apply K-means clustering to the rows of $U_K$ (i.e. treat each row of $U_K$ as a $K$-dimensional feature vector and cluster it). \n",
    "6. Return the cluster labels from step $4$. $\\mathbf{x}_i$ is assigned to the cluster which the $i$-th row of $U_K$ was assigned to. \n",
    "\n",
    "$L$ is known as the normalized Laplacian of the similarity graph, and has many nice properties for analyzing the similarity graph, most of which are beyond the scope of the course. \n",
    "\n",
    "Note: You don't really need to know why spectral clustering works to do this problem (though it would be nice) -- you just need to be able to implement the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'll make a data set based on the Illinois logo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=np.nonzero(np.asarray(\n",
    "[[1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "[1,0,0,0,0,0,0,0,0,0,0,1],\n",
    "[1,0,0,0,0,0,0,0,0,0,0,1],\n",
    "[1,0,0,1,1,1,1,1,1,0,0,1],\n",
    "[1,0,0,0,0,1,1,0,0,0,0,1],\n",
    "[1,0,0,0,0,1,1,0,0,0,0,1],\n",
    "[1,0,0,0,0,1,1,0,0,0,0,1],\n",
    "[1,0,0,1,1,1,1,1,1,0,0,1],\n",
    "[1,0,0,0,0,0,0,0,0,0,0,1],\n",
    "[1,0,0,0,0,0,0,0,0,0,0,1],\n",
    "[1,1,1,1,1,1,1,1,1,1,1,1]]))\n",
    "\n",
    "illcmap=ListedColormap(['#131F33','#FA6300'])\n",
    "\n",
    "data=np.c_[tmp[1],tmp[0]]\n",
    "figure()\n",
    "scatter(data[:,0],data[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first see what happens if we try to cluster these points using K-means to get 2 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf=KMeans(init='k-means++',n_clusters=2)\n",
    "kmf.fit(data)\n",
    "figure()\n",
    "scatter(data[:,0],data[:,1],c=kmf.labels_,cmap=illcmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see something like the right half of the points are in one cluster, and the left half are in the other. The Illinois I should not be separated from the perimeter. In general, **K-means cannot produce non-convex clusters** (i.e. if you draw a line between any 2 points in a cluster, any point that lies on that line is in that cluster), so it cannot separate the I from the border."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement spectral clustering as described above. \n",
    "\n",
    "Recall that `numpy.linalg.eigh` returns the eigenvalues of a matrix in *ascending* order.\n",
    "\n",
    "The code provided already calculates $L$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectralClustering(data,K,C=1):\n",
    "    W=np.exp(-dist.cdist(data,data,'sqeuclidean')/C)\n",
    "    W=W-np.diag(np.diag(W))\n",
    "    Dinv5=np.diag( (W.dot(np.ones(W.shape[0])))**(-0.5) )\n",
    "    L=np.eye(W.shape[0])-Dinv5.dot(W).dot(Dinv5)\n",
    "    # Put your code below\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run your spectral clustering implementation with 2 clusters on the data in `data`, and plot the data with the colors given by the clusters returned by `spectralClustering` with `cmap=illcmap`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure()\n",
    "color = spectralClustering(data,2)\n",
    "if color:\n",
    "    scatter(data[:,0],data[:,1],c=color,cmap=illcmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default value of $C=1$ in the spectral clustering code should separate the I from the border (though which will be colored orange and which will be blue will be random)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# And this concludes the Machine Learning section of the course! Good luck with your future endeavors!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

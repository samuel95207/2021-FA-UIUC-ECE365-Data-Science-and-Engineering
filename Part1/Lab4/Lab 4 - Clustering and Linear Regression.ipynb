{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Clustering and Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Your Name Here (Your netid here)\n",
    "\n",
    "### Due September 22nd, 2021 11:59 PM\n",
    "\n",
    "**Logistics and Lab Submission**\n",
    "\n",
    "See the course website. Remember that all labs count equally, despite the labs being graded from a different number of total points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What You Will Need to Know For This Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* K-means clustering\n",
    "* Vector Quantization\n",
    "* Nearest Neighbors Classification\n",
    "* Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission procedure is provided below:\n",
    "- You will be provided with a template Python script (main.py) for this lab where you need to implement the provided functions as needed for each question. Follow the instructions provided in this Jupyter Notebook (.ipynb) to implement the required functions. **Do not change the file name or the function headers!**\n",
    "- Upload only your Python script (.py file) on Gradescope. Don't upload your datasets or Jupyter Notebook (.ipynb file).\n",
    "- Your grades and feedbacks will appear on Gradescope. The grading for the programming questions is automated using Gradescope autograder, no partial credits are given. Therefore, if you wish, you will have a chance to re-submit your code **within 72 hours** of receiving your first grade for this lab, only if you have *reasonable* submissions before the deadline (i.e. not an empty script).\n",
    "- You can submit for any times you want before the deadline. After the deadline, you will see a re-submission assignment open on Gradescope, and you can still submit multiple times to that re-submission assignment within 72 hours. If you re-submit after the deadline, the final grade for the programming part of this lab will be calculated as max{first_grade, .4 \\* first_grade + .6 \\* .9 \\* re-submission_grade}.\n",
    "- This lab also has Multiple Choice Questions (MCQs) that are needed to be completed on Gradescope **within the deadline**.\n",
    "\n",
    "There are some problems which have short answer questions. They are not graded, but we are free to discuss answers to these problems. **Multiple Choice Questions (MCQs) will be graded on Gradescope!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble (don't change this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "from sklearn import neighbors\n",
    "from numpy import genfromtxt\n",
    "import scipy.spatial.distance as dist\n",
    "from sklearn.cluster import KMeans\n",
    "from PIL import Image\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run main.py\n",
    "q1 = Question1()\n",
    "q2 = Question2()\n",
    "q3 = Question3()\n",
    "q4 = Question4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Selecting the number of clusters (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1:** In function `kMeans`, you should implement the $K$-means clustering algorithm.\n",
    "\n",
    "You will be given as input:\n",
    "* A $(N,d)$ `numpy.ndarray` of unlabeled data (with each row as a feature vector), data\n",
    "* A scalar $K$ which indicates the number of clusters\n",
    "* A scalar representing the number of iterations, *niter* (this is your stopping criterion/criterion for convergence)\n",
    "\n",
    "Your output will be a tuple consisting of a vector of length $N$ containing which cluster ($0,\\ldots,K-1$) a feature vector is in and a $(K,d)$ matrix with the rows containing the cluster centers.\n",
    "\n",
    "Your code should alternatingly re-fit the cluster centers (randomly initialize in the first round) and find the corresponding labels. The above alternating process should take *niter* iterations. For grading purposes, you are also given a random seed. Do NOT change it!\n",
    "\n",
    "Do not use scikit-learn or similar for implement K-means clustering. You may use `scipy.spatial.distance.cdist` to calculate distances. Initialize the centers randomly without replacement with points from the data set. To be consistent with the autograder, you should use `np.random.choice` to randomly choose $K$ points for initialization. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2:** The K-means clustering problem tries to minimize the following quantity by selecting $\\{z_i\\}_{i=1}^N$ and $\\{\\mu_k\\}_{k=1}^K$:\n",
    "$$J_K(\\{z_i\\}_{i=1}^N ,\\{\\mu_k\\}_{k=1}^K)=\\sum_{i=1}^N \\lVert \\mathbf{x}_i - \\mathbf{\\mu}_{z_i} \\rVert^2$$\n",
    "where $\\mathbf{\\mu}_{z_i}$ is the center of the cluster to which $\\mathbf{x}_i$ is assigned.\n",
    "\n",
    "One visual heuristic to choose the number of clusters from the data (where the number of clusters is not known a priori) is to estimate the optimal value of $J_K(\\{z_i\\}_{i=1}^N ,\\{\\mu_k\\}_{k=1}^K)$, $J^*(K)$ , for different values of $K$ and look for **an \"elbow\" or \"knee\"** in the curve of $J^*$ versus $K$ and choose that value of $K$. \n",
    "\n",
    "In function `calculateJ`, you should run $K$-means for each $K=2,\\ldots,10$ and calculate $J_K(\\{z_i\\}_{i=1}^N ,\\{\\mu_k\\}_{k=1}^K)$ for the clustering given by $K$-means. Note that in practice, you can have multiple runs for each $K$ value, and take the minimum $J_K$ value (for each $K$) to get the plot.\n",
    "\n",
    "For an attempt to formalize this heuristic, see Tibshirani, Robert, Guenther Walther, and Trevor Hastie. \"Estimating the number of clusters in a data set via the gap statistic.\" Journal of the Royal Statistical Society: Series B (Statistical Methodology) 63.2 (2001): 411-423. Sometimes, an elbow does not exist in the curve or there are multiple elbows or the $K$ value of an elbow cannot be unambiguously identified. Further material can be found on <a href=\"http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_Elbow_Method\">Wikipedia</a> as well.  \n",
    "\n",
    "Note: Your code should be relatively quick -- a few minutes, at worst. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up some data, which we will store in a variable called problem1\n",
    "q1 = Question1()\n",
    "data_p1 = genfromtxt('problem1.csv', delimiter=',')\n",
    "err = q1.calculateJ(data_p1,q1.kMeans)\n",
    "plot(range(2,11),err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the value of $K$ you determined from the elbow, the following code performs K-means clustering on the data. It uses a scatter plot with the colors given by the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = # Put your K value here\n",
    "lk,ck = q1.kMeans(data_p1,K,100)\n",
    "scatter(data_p1[:,0], data_p1[:,1], c=lk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Vector Quantization (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will implement vector quantization. You will use `sklearn.cluster.KMeans` for the K-means implementation and use *k-means++* as the initialization method. See Section 4.2.1 in the notes for details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1:** Write function `trainVQ` to generate a codebook for vector quantization. You will be given inputs:\n",
    "* A $(N,M)$ `numpy.ndarray` representing a greyscale image, called *image*. (Note that in practice, if we want to generate our codebook from multiple images, we can concatenate the images before running them through this function.)\n",
    "* A scalar $B$, for which you will use $B \\times B$ blocks for vector quantization. You may assume $N$ and $M$ are divisible by $B$.\n",
    "* A scalar $K$, which is the size of your codebook\n",
    "\n",
    "You will return:\n",
    "* The codebook as a $(K,B^2)$ `numpy.ndarray`.\n",
    "\n",
    "**For grading purposes only:** Please flatten any matrix in *row-major* order. One way is to use `numpy.flatten` to flatten your matrix. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2:** Write function `compressImg` which compresses an image against a given codebook. You will be given inputs:\n",
    "* A $(N,M)$ `numpy.ndarray` representing a greyscale image, called image. You may assume $N$ and $M$ are divisible by $B$.\n",
    "* A $(K,B^2)$ codebook called *codebook*\n",
    "* Block width $B$\n",
    "\n",
    "You will return:\n",
    "* A $(N/B,M/B)$ `numpy.ndarray` consisting of the indices in the codebook used to approximate the image. \n",
    "\n",
    "You can use the nearest neighbor classifier from scikit-learn if you want (though it is not necessary) to map blocks to their nearest codeword. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3:** Write function `decompressImg` to reconstruct an image from its codebook. You will be given inputs:\n",
    "* A $(N/B,M/B)$ `numpy.ndarray` containing the indices of the codebook for each block called indices\n",
    "* A $(K,B^2)$ `numpy.ndarray` called *codebook*\n",
    "* Block width $B$\n",
    "\n",
    "You will return a $(N,M)$ `numpy.ndarray` representing the image. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The provided image is stored in image\n",
    "image = np.asarray(Image.open(\"mrtb.jpg\").convert(\"L\"))\n",
    "imshow(image, cmap = cm.Greys_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code runs your vector quantizer with $5 \\times 5$ blocks on the provided image with codebook sizes $K=2,5,10,20,50,100,200$ (i.e. it generates codebooks from this image of those sizes, compresses the image using those codebooks and reconstructs the images). It also displays (for each K) the reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = Question2()\n",
    "B = 5\n",
    "for K in [2,5,10,20,50,100,200]:\n",
    "    codebook,_ = q2.trainVQ(image,B,K)\n",
    "    cmpimg = q2.compressImg(image,codebook,B)\n",
    "    dcpimg = q2.decompressImg(cmpimg,codebook,B)\n",
    "    figure()\n",
    "    title(\"{} codewords: \".format(K))\n",
    "    imshow(dcpimg, cmap = cm.Greys_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following question is for you think about to further your understanding of these classifiers. These will not be graded. You could discuss these with other students or the TAs during the office hours.**\n",
    "\n",
    "Which code book would you pick? Why? Make sure to take into account the bits per pixel used by the compressor.\n",
    "\n",
    "Note the number of bits per pixel can be approximated as $\\frac{\\log_2 K}{25}$ and the codebook takes approximately $200K$ bits (assuming each pixel is stored as 8 bits). Some good ideas on quantitative arguments for codebook size can be found in Gonzalez & Woods, Digital Image Processing 3e or Gersho & Gray, Signal Compression & Vector Quantization. It is not necessary to look at these references for quantitative arguments, though.\n",
    "\n",
    "The image used is under fair use from [Daily Illini](https://dailyillini.com/special-sections/international-student-guide/2018/08/15/how-to-dress-for-the-midwestern-weather/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Using K-means to Accelerate Nearest Neighbors (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will use K-means clustering to accelerate nearest neighbors, as outlined in the notes (Algorithm 7). Use `sklearn.neighbors.KNeighborsClassifier` for nearest neighbor classification and `sklearn.cluster.KMeans` for the K-means implementation with *k-means++* as the initialization method.\n",
    "\n",
    "You will write a function to generate prototypes from labeled data. It will have input:\n",
    "* Training features as $(N,d)$ `numpy.ndarray` called *traindata*\n",
    "* Training labels as a length $N$ vector called *trainlabels*\n",
    "* $K\\_list$, a list of the number of prototypes under each class\n",
    "\n",
    "You will return a tuple of two lists:\n",
    "* A length $len(K\\_list)$ list containing the prototypes selected. The $j$-th element in the list is a ($K\\_list[j] * \\text{num_classes}, d)$ `numpy.ndarray`, representing the prototypes selected if using $K\\_list[j]$ prototypes under each class. You should keep the order as in the given $K\\_list$.\n",
    "* A length $len(K\\_list)$ list containing the prototypes selected. The $j$-th element in the list is a ($K\\_list[j] * \\text{num_classes},)$ `numpy.array`, representing the corresponding labels if using $K\\_list[j]$ prototypes under each class. You should keep the order as in the given $K\\_list$.\n",
    "\n",
    "You may assume there are at least $\\min(K\\_list)$ examples under each class. `set(trainlabels)` or `numpy.unique` will give you the set of labels. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train a nearest neighbor classifier (i.e. 1-NN)  according to a given list of K prototypes per class for the digits data set from Lab 2. Return the validation error for each K value (in the same order as the given K_list). <b>(10 points)</b>\n",
    "\n",
    "\n",
    "Note that this data set is generated from zip code digits from US mail, and the US Postal Service processes <a href=\"https://about.usps.com/who-we-are/postal-facts/one-day-by-the-numbers.htm\">hundreds of millions of pieces of mail</a> a day, so a small improvement in error can lead to tremendous savings in terms of mis-routed packages (which cost a lot of money and time to re-transport)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digits data set\n",
    "\n",
    "#Read in the Training Data\n",
    "traindata_tmp= genfromtxt('zip.train', delimiter=' ')\n",
    "#The training labels are stored in \"trainlabels\", training features in \"traindata\"\n",
    "trainlabels=traindata_tmp[:,0]\n",
    "traindata=traindata_tmp[:,1:]\n",
    "\n",
    "\n",
    "#Read in the Validation Data\n",
    "valdata_tmp= genfromtxt('zip.val', delimiter=' ')\n",
    "#The validation labels are stored in \"vallabels\", validation features in \"valdata\"\n",
    "vallabels=valdata_tmp[:,0]\n",
    "valdata=valdata_tmp[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = Question3()\n",
    "K_list = [1,10,50,100,200]\n",
    "proto_dat_list, proto_lab_list = q3.generatePrototypes(traindata,trainlabels,K_list)\n",
    "protoerr = q3.protoValError(proto_dat_list,proto_lab_list,valdata,vallabels)\n",
    "for k,e in zip(K_list,protoerr):\n",
    "    print(\"Validation Error for %s prototypes: %s\" % (k, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Linear Regression (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will do model selection for linear regression using Ordinary Least Squares, Ridge Regression and the LASSO.\n",
    "\n",
    "The dataset you will use has 8 features:\n",
    "\n",
    "    lcavol - log cancer volume\n",
    "    lcaweight - log prostate weight\n",
    "    age\n",
    "    lbph - log of amount of benign prostatic hyperplasia\n",
    "    svi - seminal vesicle invasion\n",
    "    lcp - log capsular penetration\n",
    "    gleason - Gleason score\n",
    "    pgg45 - percent of Gleason scores 4 or 5\n",
    "\n",
    "and you will predict the level of a prostate-specific antigen. The data set was collected from a set of men about to receive a radical prostatectomy. More details are given in Section 3.2.1 in Elements of Statistical Learning 2e by Hastie et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "trainp= genfromtxt('trainp.csv', delimiter=',')\n",
    "\n",
    "# Training data: \n",
    "trainfeat=trainp[:,:-1] #Training features (rows are feature vectors)\n",
    "trainresp=trainp[:,-1] #Training responses\n",
    "\n",
    "valp= genfromtxt('valp.csv',delimiter=',')\n",
    "# Validation data:\n",
    "valfeat=valp[:,:-1] #Validation Features (rows are feature vectors)\n",
    "valresp=valp[:,-1] #Validation Response\n",
    "\n",
    "# Standardize and center the features\n",
    "ftsclr=StandardScaler()\n",
    "trainfeat = ftsclr.fit_transform(trainfeat)\n",
    "valfeat= ftsclr.transform(valfeat)\n",
    "# and the responses (note that the example in the notes has centered but not \n",
    "#                    standardized responses, so your numbers won't match up)\n",
    "rsclr=StandardScaler()\n",
    "trainresp = (rsclr.fit_transform(trainresp.reshape(-1,1))).reshape(-1)\n",
    "valresp= (rsclr.transform(valresp.reshape(-1,1))).reshape(-1)\n",
    "q4 = Question4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1:** Since we centered the responses, we can begin with a **benchmark** model: *Always predict the response as zero*. (If not centered, this would predict the mean response on the training data.) In function `benchmarkRSS`, calculate the validation RSS for this model. **(5 points)**\n",
    "\n",
    "If another model does worse than this, it is a sign that something is amiss.\n",
    "\n",
    "Note: The RSS on a data set with $V$ samples is given by $\\frac{1}{V} \\lVert \\mathbf{y} - \\hat{\\mathbf{y}} \\rVert^2$ where $\\mathbf{y}$ is a vector of the responses, and $\\hat{\\mathbf{y}}$ is the predicted responses on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The benchmark validation RSS is:\", q4.benchmarkRSS(trainfeat,trainresp,valfeat,valresp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2:** Next, you will try (Ordinary) Least Squares. In function `OLSRSS`, use `sklearn.linear_model.LinearRegression` with the default options and calculate the validation RSS. <b>(5 points)</b>\n",
    "\n",
    "*Note: The .score() method returns an [$R^2$  value](https://en.wikipedia.org/wiki/Coefficient_of_determination), not the RSS, so you shouldn't use it anywhere in this problem.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The OLS validation RSS is:\", q4.OLSRSS(trainfeat,trainresp,valfeat,valresp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3:** Then, in function `RidgeRSS`, you will apply ridge regression with `sklearn.linear_model.Ridge`. In the funciton, sweep the regularization/tuning parameter $\\alpha=0,\\ldots,100$ with 1000 equally spaced values. \n",
    "\n",
    "The following code makes a plot of the RSS on the validation set versus $\\alpha$. What is the minimizing $\\alpha$, corresponding coefficients and validation error?\n",
    "\n",
    "*Note: Larger values of $\\alpha$ shrink the weights in the model more. $\\alpha=0$ corresponds to the LS solution.* <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_array, best_a, best_rss, coef = q4.RidgeRSS(trainfeat,trainresp,valfeat,valresp)\n",
    "print(\"The minimizing alpha is: %s\" % best_a)\n",
    "print(\"The best coefficients are:\")\n",
    "print(coef)\n",
    "print(\"The best validation RSS is: %s\" % best_rss)\n",
    "a = np.linspace(0,100,1000)\n",
    "plot(a,rss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4:** Now, you will apply the LASSO with `sklearn.linear_model.Lasso`. In function `LassoRSS`, sweep the tuning/regularization parameter $\\alpha=0,\\ldots,1$ with 1000 equally spaced values. \n",
    "\n",
    "The following code makes makes a plot of the RSS on the validation set versus $\\alpha$. What is the minimizing $\\alpha$, corresponding coefficients and validation error?\n",
    "\n",
    "*Note: Larger values of $\\alpha$ lead to sparser solutions (i.e. less features used in the model), with a sufficiently large value of $\\alpha$ leading to a constant prediction. Small values of $\\alpha$ are closer to the LS solution, with $\\alpha=0$ being the LS solution.* <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_array, best_a, best_rss, coef = q4.LassoRSS(trainfeat,trainresp,valfeat,valresp)\n",
    "print(\"The minimizing alpha is: %s\" % best_a)\n",
    "print(\"The best coefficients are:\")\n",
    "print(coef)\n",
    "print(\"The best validation RSS is: %s\" % best_rss)\n",
    "a = np.linspace(0.00001,1,1000)\n",
    "plot(a,rss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And this concludes Lab 4! Congratulations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
